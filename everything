{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "#client_id: jhAffV3EzTf80A\n",
    "#secret: f38Y5S9QTzm6y0LyG5UiU47sMKk\n",
    "#username: carameldeligh5\n",
    "#password: @Collingsworth5\n",
    "\n",
    "#scrapes google for top 10 links \n",
    "\n",
    "# query = \"best places to eat in Austin reddit\"\n",
    "query = \"best places to eat in seattle reddit\"\n",
    "substring = \"comments/\"\n",
    "\n",
    "#FUNCTIONS \n",
    "\n",
    "#returns a list of post IDs \n",
    "def get_postIDs(query): \n",
    "    \n",
    "    page = requests.get(\"https://www.google.com/search?q=\" + query)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    urls = soup.select(\".r a\")\n",
    "    links = [\"https://www.google.com\" + url.get('href') for url in urls]\n",
    "    \n",
    "    #could be extra processing here (LOOK DOWN)\n",
    "    links = links[1:]\n",
    "    \n",
    "    submissions = []\n",
    "    for link in links:\n",
    "        if substring in link: \n",
    "            begIndex = link.index(substring) + 9\n",
    "            endIndex = begIndex + 6\n",
    "            submissions.append(link[begIndex : endIndex])\n",
    "    \n",
    "    return submissions\n",
    "        \n",
    "\n",
    "postIDs = get_postIDs(query)\n",
    "\n",
    "client_id = \"jhAffV3EzTf80A\"\n",
    "client_secret = \"f38Y5S9QTzm6y0LyG5UiU47sMKk\"\n",
    "username = \"carameldelight5\"\n",
    "password = \"@Collingsworth5\"\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     password=password,\n",
    "                     user_agent='testscript by /u/carameldelight5',\n",
    "                     username=username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8bjzcb', '7qw2px', '6n9xbi', '8l3nru', '7xzpst']\n",
      "17\n",
      "ITEM: Sushi Kashiba  SCORE: 15\n",
      "ITEM: Shiro Kashiba's  SCORE: 15\n",
      "ITEM: Thai Tom - $  SCORE: 15\n",
      "ITEM: Stacia  SCORE: 15\n",
      "ITEM: Pagliacci Pizza - $  SCORE: 15\n",
      "ITEM: Roccos Pizza  SCORE: 3\n",
      "ITEM: Masonry Pizza  SCORE: 3\n",
      "ITEM: Pub  SCORE: 3\n",
      "ITEM: Hot Mama's  SCORE: 8\n",
      "ITEM: Radiator Whiskey  SCORE: 3\n",
      "ITEM: Tacos Chukis  SCORE: 3\n",
      "ITEM: Kylies  SCORE: 3\n",
      "ITEM: Donuts  SCORE: 3\n",
      "ITEM: Molly Moons  SCORE: 3\n",
      "ITEM: Gourmet Noodle Bowl  SCORE: 28\n",
      "ITEM: Jebena Cafe  SCORE: 28\n",
      "ITEM: Dick  SCORE: 25\n"
     ]
    }
   ],
   "source": [
    "# SPACY \n",
    "spacyNLP = spacy.load(\"en\")\n",
    "\n",
    "#holds item & upvote score \n",
    "items = [] \n",
    "\n",
    "#gets the specified number of posts & comments \n",
    "numPosts = 5 \n",
    "numComments = 3\n",
    "postIDs = postIDs[0:numPosts]\n",
    "print (postIDs)\n",
    "\n",
    "#iterates through posts \n",
    "for postID in postIDs: \n",
    "    \n",
    "    post = reddit.submission(postID)\n",
    "    comments = post.comments[0:numComments]\n",
    "    \n",
    "    #iterates through comments & extracts relevant entities \n",
    "    for comment in comments: \n",
    "        commentNLP = spacyNLP(comment.body)\n",
    "        \n",
    "        for ent in commentNLP.ents: \n",
    "            if ent.label_ == \"PERSON\": \n",
    "                data = (ent.text, comment.score)\n",
    "                items.append(data)\n",
    "\n",
    "\n",
    "print(len(items))\n",
    "for item in items: \n",
    "    print (\"ITEM:\", item[0], \" SCORE:\", item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMMENT:  For pizza you should check out a Big Mario's. My partner is from the east coast and she hates all of our pizza except for there and Hot Mama's.\n",
      "(PERSON Mario/NNP)\n",
      "(PERSON Hot/NNP Mama/NNP)\n",
      "\n",
      "COMMENT:  BBQ for me would be Jacks BBQ or Radiator Whiskey. Tacos I would head to Tacos Chukis. Burgers I would go to Uneeda. Pizza is a toss up, if you want deep dish try Kylies in Fremont, for thin crust there are millions but Albi room is my personal favorite.  Coffee is Victrola by a longshot.  Donuts is Mighty O all the way.   I am not a huge dessert person but Molly Moons is always high on my ice cream list. \n",
      "\n",
      "(ORGANIZATION BBQ/NNP Radiator/NNP Whiskey/NNP)\n",
      "(PERSON Tacos/NNP Chukis/NNP)\n",
      "(PERSON Uneeda/NNP)\n",
      "(GPE Pizza/NNP)\n",
      "(PERSON Kylies/NNP Fremont/NNP)\n",
      "(PERSON Albi/NNP)\n",
      "(PERSON Coffee/NNP)\n",
      "(ORGANIZATION Victrola/NNP)\n",
      "(PERSON Donuts/NNP)\n",
      "(PERSON Mighty/NNP O/NNP)\n",
      "(PERSON Molly/NNP Moons/NNP)\n",
      "\n",
      "COMMENT:  BBQ - Wood Shop BBQ on Jackson\n",
      "\n",
      "Burgers - Red Mill\n",
      "\n",
      "Mexican (Tacos) - Tacos El Asadero\n",
      "\n",
      "Pizza - Sizzle Pie\n",
      "\n",
      "(ORGANIZATION BBQ/NNP)\n",
      "(PERSON Wood/NN Shop/NNP)\n",
      "(PERSON Jackson/NNP Burgers/NNP)\n",
      "(PERSON Red/NNP Mill/NNP Mexican/NNP)\n",
      "(ORGANIZATION Tacos/NNP)\n",
      "(PERSON Tacos/NN El/NNP Asadero/NNP Pizza/NNP)\n",
      "(PERSON Sizzle/NNP Pie/NNP)\n",
      "\n",
      "COMMENT:  I wouldn't use Austin categories.  Different place, I wouldn't even put BBQ or breakfast tacos in there.  You gotta include pho, sushi, Thai, and maybe Indian.  \n",
      "\n",
      "Anyway...\n",
      "\n",
      "Burgers:  Zippy's\n",
      "\n",
      "Mexican:  Fonda la Catarina\n",
      "\n",
      "Pizza:  Pagliacci (too bad Red Star closed)\n",
      "\n",
      "Coffee:  Uptown Espresso\n",
      "\n",
      "Hot Dogs:  blech\n",
      "\n",
      "Dessert:  Full Tilt ice cream! \n",
      "\n",
      "Now to the real categories: \n",
      "\n",
      "Pho:  Pho 99 or Pho Hi-Ho\n",
      "\n",
      "Sushi:  Shiro's \n",
      "\n",
      "Thai:  dunno\n",
      "\n",
      "Indian:  I've heard the best places are on the East side and I don't ever go there.\n",
      "\n",
      "\n",
      "(GPE Austin/JJ)\n",
      "(ORGANIZATION BBQ/NNP)\n",
      "(GPE Thai/NNP)\n",
      "(GPE Indian/JJ)\n",
      "(PERSON Zippy/NNP)\n",
      "(GPE Mexican/JJ)\n",
      "(ORGANIZATION Catarina/NNP)\n",
      "(PERSON Uptown/JJ Espresso/NNP Hot/NNP Dogs/NNP)\n",
      "(PERSON Full/NNP Tilt/NNP)\n",
      "(PERSON Shiro/NNP)\n",
      "(GPE Thai/NN)\n",
      "(GPE Indian/JJ)\n",
      "(GPE East/NNP)\n",
      "\n",
      "COMMENT:  Check out Jack's BBQ on Airport Way!\n",
      "(PERSON Check/NNP)\n",
      "(PERSON Jack/NNP)\n",
      "(ORGANIZATION BBQ/NNP Airport/NNP Way/NNP)\n"
     ]
    }
   ],
   "source": [
    "#NLTK TEST \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "commentIndex = 0\n",
    "while commentIndex < 5: \n",
    "    \n",
    "    dummyComment = testComments[commentIndex].body\n",
    "    print(\"\\nCOMMENT: \", dummyComment)\n",
    "    dummyComment = ' '.join([word for word in dummyComment.split() if word not in stop])\n",
    "    sentences = nltk.sent_tokenize(dummyComment)\n",
    "\n",
    "    #tokenizes sentences into words --> sentences will become a 2D list [ [blah, blah, blah], [blah, blah, blah] ]\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    #tags each word in a sentence with a \"part of speech\" label --> sentences will become a 2D list with tuples \n",
    "    # --> [ [ (blah, yuh), (blah, yuh), (blah, yuh) ], [ (blah, yuh), (blah, yuh), (blah, yuh) ] ]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "    #stuff here \n",
    "    for tagged_sentence in sentences: \n",
    "        for chunk in nltk.ne_chunk(tagged_sentence): \n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                print (chunk)\n",
    "                \n",
    "    commentIndex += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7qw2px\n",
      "<class 'list'>\n",
      "('Hmm', 'NNP')\n",
      "('some', 'DT')\n",
      "('personal', 'JJ')\n",
      "('suggestions', 'NNS')\n",
      "('Japanese', 'JJ')\n",
      "('*', 'NNP')\n",
      "('Sushi', 'NNP')\n",
      "('Kappo', 'NNP')\n",
      "('Tamura', 'NNP')\n",
      "('Great', 'NN')\n",
      "('for', 'IN')\n",
      "('fresh', 'JJ')\n",
      "('sustainable', 'JJ')\n",
      "('fish', 'NN')\n",
      "('and', 'CC')\n",
      "('stand-out', 'NN')\n",
      "('cooked', 'JJ')\n",
      "('dishes', 'NNS')\n",
      "('in', 'IN')\n",
      "('particular', 'JJ')\n",
      "('chawan', 'NN')\n",
      "('mushi', 'NN')\n",
      "('and', 'CC')\n",
      "('kamayaki', 'NN')\n",
      "('*', 'NN')\n",
      "('Sushi', 'NNP')\n",
      "('Kashiba', 'NNP')\n",
      "('Shiro', 'NNP')\n",
      "('Kashiba', 'NNP')\n",
      "(\"'s\", 'POS')\n",
      "('current', 'JJ')\n",
      "('establishment', 'NN')\n",
      "('perhaps', 'RB')\n",
      "('the', 'DT')\n",
      "('finest', 'JJS')\n",
      "('sushi', 'NN')\n",
      "('you', 'PRP')\n",
      "('can', 'MD')\n",
      "('get', 'VB')\n",
      "('in', 'IN')\n",
      "('Seattle', 'NNP')\n",
      "('if', 'IN')\n",
      "('cost', 'NN')\n",
      "('is', 'VBZ')\n",
      "('no', 'DT')\n",
      "('issue', 'NN')\n",
      "('*', 'NNP')\n",
      "('Musashi', 'NNP')\n",
      "(\"'s\", 'POS')\n",
      "('The', 'DT')\n",
      "('only', 'JJ')\n",
      "('cheap', 'JJ')\n",
      "('sushi', 'NN')\n",
      "('worth', 'IN')\n",
      "('getting', 'VBG')\n",
      "('an', 'DT')\n",
      "('excellent', 'JJ')\n",
      "('chirashi', 'NN')\n",
      "('bowl', 'NN')\n",
      "('Thai', 'NNP')\n",
      "('*', 'NNP')\n",
      "('Little', 'NNP')\n",
      "('Thai', 'NNP')\n",
      "('Restaurant', 'NNP')\n",
      "('Hidden', 'NNP')\n",
      "('in', 'IN')\n",
      "('a', 'DT')\n",
      "('basement', 'NN')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('UDistrict', 'NNP')\n",
      "('flavorful', 'JJ')\n",
      "('with', 'IN')\n",
      "('great', 'JJ')\n",
      "('proportions', 'NNS')\n",
      "('I', 'PRP')\n",
      "('perhaps', 'RB')\n",
      "('like', 'IN')\n",
      "('their', 'PRP$')\n",
      "('wide', 'JJ')\n",
      "('noodle', 'NN')\n",
      "('and', 'CC')\n",
      "('fried', 'JJ')\n",
      "('rice', 'NN')\n",
      "('dishes', 'VBZ')\n",
      "('the', 'DT')\n",
      "('best', 'JJS')\n",
      "('*', 'JJ')\n",
      "('Thai', 'NNP')\n",
      "('Tom', 'NNP')\n",
      "('Fresh', 'NNP')\n",
      "('stir', 'NN')\n",
      "('fry', 'NN')\n",
      "('dishes', 'NNS')\n",
      "('are', 'VBP')\n",
      "('done', 'VBN')\n",
      "('best', 'JJS')\n",
      "('here', 'RB')\n",
      "('in', 'IN')\n",
      "('particular', 'JJ')\n",
      "('phad', 'NN')\n",
      "('thai', 'NN')\n",
      "('and', 'CC')\n",
      "('spicy', 'NN')\n",
      "('broccoli', 'NN')\n",
      "('Also', 'RB')\n",
      "('a', 'DT')\n",
      "('free', 'JJ')\n",
      "('show', 'NN')\n",
      "('if', 'IN')\n",
      "('you', 'PRP')\n",
      "('can', 'MD')\n",
      "('get', 'VB')\n",
      "('a', 'DT')\n",
      "('seat', 'NN')\n",
      "('*', 'JJ')\n",
      "('Thai', 'NNP')\n",
      "('Curry', 'NNP')\n",
      "('Simple', 'NNP')\n",
      "('Simple', 'NN')\n",
      "('curry', 'NN')\n",
      "('with', 'IN')\n",
      "('cheap', 'JJ')\n",
      "('ingredients', 'NNS')\n",
      "('which', 'WDT')\n",
      "('are', 'VBP')\n",
      "('carried', 'VBN')\n",
      "('on', 'IN')\n",
      "('the', 'DT')\n",
      "('strength', 'NN')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "('underlying', 'VBG')\n",
      "('curry', 'NN')\n",
      "('sauces', 'NNS')\n",
      "('Pizza', 'NNP')\n",
      "('*', 'NNP')\n",
      "('Can', 'NNP')\n",
      "('Am', 'NNP')\n",
      "('Pizza', 'NNP')\n",
      "('Standout', 'NN')\n",
      "('for', 'IN')\n",
      "('their', 'PRP$')\n",
      "('Indian', 'JJ')\n",
      "('takes', 'VBZ')\n",
      "('on', 'IN')\n",
      "('pizza', 'NN')\n",
      "('in', 'IN')\n",
      "('particular', 'JJ')\n",
      "('butter', 'NN')\n",
      "('chicken', 'NN')\n",
      "('pizza', 'NN')\n",
      "('*', 'JJ')\n",
      "('Stacia', 'NNP')\n",
      "(\"'s\", 'POS')\n",
      "('Gourmet', 'NNP')\n",
      "('Pizza', 'NNP')\n",
      "('&', 'CC')\n",
      "('Pasta', 'NNP')\n",
      "('Excellent', 'NN')\n",
      "('large', 'JJ')\n",
      "('NY-style', 'JJ')\n",
      "('pizza', 'NN')\n",
      "('Large', 'JJ')\n",
      "('variety', 'NN')\n",
      "('of', 'IN')\n",
      "('toppings', 'NNS')\n",
      "('sauces', 'NNS')\n",
      "('and', 'CC')\n",
      "('crusts', 'NNS')\n",
      "('*', 'VB')\n",
      "('A', 'NNP')\n",
      "('Pizza', 'NNP')\n",
      "('Mart', 'NNP')\n",
      "('Pizza', 'NN')\n",
      "('by', 'IN')\n",
      "('the', 'DT')\n",
      "('slice', 'NN')\n",
      "('and', 'CC')\n",
      "('beers', 'NNS')\n",
      "('and', 'CC')\n",
      "('other', 'JJ')\n",
      "('drinks', 'NNS')\n",
      "('on', 'IN')\n",
      "('tap', 'NN')\n",
      "('*', 'JJ')\n",
      "('Pagliacci', 'NNP')\n",
      "('Pizza', 'NNP')\n",
      "('Regional', 'JJ')\n",
      "('delivery', 'NN')\n",
      "('pizza', 'NN')\n",
      "('institution', 'NN')\n",
      "('Also', 'RB')\n",
      "('nice', 'JJ')\n",
      "('sit-down', 'JJ')\n",
      "('locations', 'NNS')\n",
      "('*', 'JJ')\n",
      "('Zeek', 'NNP')\n",
      "(\"'s\", 'POS')\n",
      "('Pizza', 'NNP')\n",
      "('Other', 'JJ')\n",
      "('regional', 'JJ')\n",
      "('delivery', 'NN')\n",
      "('pizza', 'NN')\n",
      "('institution', 'NN')\n",
      "('Has', 'VBZ')\n",
      "('some', 'DT')\n",
      "('well-executed', 'JJ')\n",
      "('non-standard', 'JJ')\n",
      "('pizzas', 'NN')\n",
      "('in', 'IN')\n",
      "('particular', 'JJ')\n",
      "('the', 'DT')\n",
      "('Thai', 'NNP')\n",
      "('pizza', 'NN')\n",
      "('Fried', 'NNP')\n",
      "('Chicken', 'NNP')\n",
      "('*', 'NNP')\n",
      "('Fat', 'NNP')\n",
      "('’', 'NNP')\n",
      "('s', 'NNP')\n",
      "('Chicken', 'NNP')\n",
      "('&', 'CC')\n",
      "('Waffles', 'NNP')\n",
      "('Large', 'NN')\n",
      "('but', 'CC')\n",
      "('tender', 'NN')\n",
      "('and', 'CC')\n",
      "('juicy', 'NN')\n",
      "('fried', 'VBN')\n",
      "('chicken', 'NN')\n",
      "('with', 'IN')\n",
      "('a', 'DT')\n",
      "('thin', 'JJ')\n",
      "('crispy', 'JJ')\n",
      "('crust', 'NN')\n",
      "('However', 'RB')\n",
      "('not', 'RB')\n",
      "('a', 'DT')\n",
      "('dedicated', 'VBN')\n",
      "('fried', 'JJ')\n",
      "('chicken', 'NN')\n",
      "('establishment', 'NN')\n",
      "('*', 'JJ')\n",
      "('Quick', 'NNP')\n",
      "('Pack', 'NNP')\n",
      "('Food', 'NNP')\n",
      "('Mart', 'NNP')\n",
      "('This', 'DT')\n",
      "('shady-looking', 'JJ')\n",
      "('convenience', 'NN')\n",
      "('store', 'NN')\n",
      "('houses', 'NNS')\n",
      "('some', 'DT')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "('best', 'JJS')\n",
      "('fried', 'JJ')\n",
      "('chicken', 'NN')\n",
      "('in', 'IN')\n",
      "('Seattle', 'NNP')\n",
      "('A', 'DT')\n",
      "(\"that's-not-a-breast\", 'JJ')\n",
      "(\"-that's-the-entire-upper-torso\", 'JJ')\n",
      "('fried', 'VBN')\n",
      "('chicken', 'NN')\n",
      "('breast', 'NN')\n",
      "('Recommend', 'VB')\n",
      "('the', 'DT')\n",
      "('wings', 'NNS')\n",
      "('*', 'NN')\n",
      "('Sisters', 'NNS')\n",
      "('And', 'CC')\n",
      "('Brothers', 'NNPS')\n",
      "('Heavily', 'NNP')\n",
      "('seasoned', 'VBD')\n",
      "('pieces', 'NNS')\n",
      "('are', 'VBP')\n",
      "('packed', 'VBN')\n",
      "('with', 'IN')\n",
      "('flavor', 'NN')\n",
      "('Recommend', 'VB')\n",
      "('the', 'DT')\n",
      "('large', 'JJ')\n",
      "('pieces', 'NNS')\n",
      "('breasts', 'NNS')\n",
      "('and', 'CC')\n",
      "('thighs', 'NNS')\n"
     ]
    }
   ],
   "source": [
    "# TEXTBLOB TEST\n",
    "from textblob import TextBlob\n",
    "\n",
    "#gets comment #1 from post #1 \n",
    "postID = postIDs[0]\n",
    "print(postID)\n",
    "post = reddit.submission(postID)\n",
    "comment = post.comments[0]\n",
    "blobComment = TextBlob(comment.body)\n",
    "\n",
    "nextPos = 0 \n",
    "currentPos = 0\n",
    "max = len(blobComment.tags)\n",
    "for tag in blobComment.tags: \n",
    "    print (tag)\n",
    "    \n",
    "#if tag[index + 1] != \"NNP\"\n",
    "#if tag[index + 1] == \"NNP\"\n",
    "isNNP = True \n",
    "if (isNNP) { \n",
    "    \n",
    "}\n",
    "    \n",
    "# print(blobComment.tags)\n",
    "# print(blobComment.noun_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
